{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127aba85-77ac-409c-9b0f-864e541e2f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "import json\n",
    "\n",
    "dbutils.widgets.text(\"dt\", \"2026-01-25\")\n",
    "dbutils.widgets.text(\"bucket\", \"\")\n",
    "dbutils.widgets.text(\"bronze_prefix\", \"raw/optio\")\n",
    "dbutils.widgets.text(\"silver_prefix\", \"silver/optio\")\n",
    "dbutils.widgets.dropdown(\"table\", \"chain_daily_state\", [\"chain_daily_state\", \"active_locks_daily\"])\n",
    "\n",
    "dt = dbutils.widgets.get(\"dt\")\n",
    "bucket = dbutils.widgets.get(\"bucket\").strip()\n",
    "bronze_prefix = dbutils.widgets.get(\"bronze_prefix\").strip().rstrip(\"/\")\n",
    "silver_prefix = dbutils.widgets.get(\"silver_prefix\").strip().rstrip(\"/\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "\n",
    "if not bucket:\n",
    "    raise ValueError(\"bucket is required\")\n",
    "\n",
    "bronze_dt = f\"s3://{bucket}/{bronze_prefix}/dt={dt}\"\n",
    "manifest_path = f\"{bronze_dt}/_MANIFEST.json\"\n",
    "\n",
    "\n",
    "\n",
    "def read_manifest(path: str) -> dict:\n",
    "    txt = spark.read.text(path).first()[\"value\"]\n",
    "    return json.loads(txt)\n",
    "\n",
    "manifest = read_manifest(manifest_path)\n",
    "datasets = manifest[\"datasets\"]\n",
    "snapshot_height = int(manifest[\"snapshot_height\"])\n",
    "manifest_created_at = manifest.get(\"created_at\")  # ISO string\n",
    "\n",
    "def dataset_run_path(name: str) -> str:\n",
    "    info = datasets.get(name)\n",
    "    if not info or info.get(\"status\") != \"SUCCESS\":\n",
    "        raise ValueError(f\"Dataset not SUCCESS in manifest: {name}\")\n",
    "    return f\"{bronze_dt}/dataset={name}/run_id={info['run_id']}\"\n",
    "\n",
    "def read_jsonl_gz(path: str):\n",
    "    return spark.read.json(f\"{path}/part-*.jsonl.gz\")\n",
    "\n",
    "\n",
    "def write_delta_partition(df, table_name: str):\n",
    "    out_path = f\"s3://{bucket}/{silver_prefix}/{table_name}\"\n",
    "    (df.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"dt = '{dt}'\")\n",
    "        .partitionBy(\"dt\")\n",
    "        .save(out_path))\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def build_active_locks_daily():\n",
    "    run_id = datasets[\"lockup_active_locks\"][\"run_id\"]\n",
    "    locks = read_jsonl_gz(dataset_run_path(\"lockup_active_locks\"))\n",
    "\n",
    "    df = (locks.select(\n",
    "            F.lit(dt).alias(\"dt\"),\n",
    "            F.col(\"snapshot_height\").cast(\"bigint\").alias(\"height\"),\n",
    "            F.col(\"extracted_at\").cast(\"timestamp\").alias(\"snapshot_ts\"),\n",
    "            F.col(\"address\").cast(\"string\").alias(\"address\"),\n",
    "            F.to_date(\"unlock_date\").alias(\"unlock_date\"),\n",
    "            F.col(\"amount_denom\").cast(\"string\").alias(\"denom\"),\n",
    "            F.col(\"amount_uopt\").cast(T.DecimalType(38,0)).alias(\"amount_uopt\"),\n",
    "            F.lit(run_id).alias(\"source_run_id\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"lock_id\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\"|\",\n",
    "                    F.col(\"dt\"),\n",
    "                    F.col(\"height\").cast(\"string\"),\n",
    "                    F.col(\"address\"),\n",
    "                    F.col(\"unlock_date\").cast(\"string\"),\n",
    "                    F.col(\"denom\"),\n",
    "                    F.col(\"amount_uopt\").cast(\"string\"),\n",
    "                ),\n",
    "                256\n",
    "            )\n",
    "        )\n",
    "        .dropDuplicates([\"dt\", \"lock_id\"])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = build_active_locks_daily()\n",
    "path = write_delta_partition(df, \"silver_active_locks_daily\")\n",
    "\n",
    "print(\"Wrote:\", path)\n",
    "display(df.limit(20))\n",
    "display(df.groupBy(\"dt\").count())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": "A10",
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_silver_notebook_build_optio",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
